{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StarWarsGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xoyh6iJjTMpG",
        "wOGvKlFG6OmU",
        "GoAHbE_aqGD4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoyh6iJjTMpG"
      },
      "source": [
        "# Star Warsify yourself!\n",
        "![Alt Text](https://miro.medium.com/max/1400/1*Xp6q6b1kMb2Y5wrpfUi1uA.gif)\n",
        "\n",
        "This notebook demonstrates how to use state-of-the-art machine learning techniques with AMD compute accelerators and Rocm for training by modifying Nvidia's styleGun2.\n",
        "\n",
        "## What is styleGun2.\n",
        "styleGun2 was developed by Nvidia's research department and currently represents the state of art in synthetic image generation. It is capable of generating photorealistic images that are indistinguishable from real image. It is based on the Generative Adversarial Network (GAN) architecture. What is GAN? It is two networks trying to compete with each other: the generator and the discriminator. The generator tries to fool the discriminator into thinking its generated image is real, while the discriminator tries to distinguish between the real image and the fake (generated) image.\n",
        "\n",
        "![Alt Text](https://miro.medium.com/max/1741/1*t78gwhhw-hn1CgXc1K89wA.png)\n",
        "\n",
        "The discriminator is first trained by showing it the real image from the dataset and random noise (image from the untrained generator). Since the data distribution is very different, the discriminator will be able to distinguish easily.\n",
        "\n",
        "We will then proceed to train the generator while freezing the discriminator. The generator will learn how to produce better images based on the discriminator's output (real or fake) until the discriminator can no longer discriminate correctly. Then we switch back to training the discriminator, and the cycle continues, both getting better until the generator reaches the point of producing a very realistic image, and training can be stopped.\n",
        "\n",
        "\n",
        "Now that we've developed some understanding of what a gan is, let's go back to NVIDIA's styleGun2 and see what makes it unique and allows it to perform so well; One of the innovations is that it unravels latent space, which will enable us to control attributes at a different level. For example, the lower level might control the pose and head shape, while the higher-level controls higher-level features such as lighting or texture. The disentanglement is done by introducing an additional mapping network that maps the input z (noise/*random* vector sampled from a normal distribution) to a separate vector w and feeds it to different layers. Consequently, each part of the z input controls a different layer of features\n",
        "\n",
        "![Alt Text](https://miro.medium.com/max/3484/1*iTf9srf5HczqVv26U7Ah_Q.png)\n",
        "So if we want to change some features in the generated image, we can simply change the inputs in the lower layers to achieve variations in the high-level features, such as the head shape, hairstyle, and pose. This will come in handy later in our project.\n",
        "\n",
        "![Alt Text](https://miro.medium.com/max/1400/0*eDxrKvi8mgkcrEk2.png)\n",
        "\n",
        "In addition, we can also modify the input of the higher layers (higher resolution 256X256 example) we can get variations in the finer features like the light intensity, skin, hair colour etc.\n",
        "\n",
        "![Alt Text](https://miro.medium.com/max/1400/0*hdRS8wWRg7g5g3fW.png)\n",
        "\n",
        "\n",
        "Moreover, we can visualise the disentanglement to further illustrate which layers control which features of the image of with spherical K-Means clustering.\n",
        "\n",
        "\n",
        "![Alt Text](https://miro.medium.com/max/1158/1*gL7_JDI-cXU9CSEoevoZSg.png)\n",
        "\n",
        "The colour represents a formed cluster, which you can think of as the controllable part of the image. In the last layer, you can see that different lighting parts are represented as separate clusters. In the middle layers, the facial features such as eye, nose or mouth are described as other clusters, which means that the variations of facial features are controlled here. Finally, in the first layers, different head parts are represented as different clusters, which proves that the shape, pose, and hairstyle of the person are controlled here.\n",
        "\n",
        "\n",
        "## StyleGAN Network Blending\n",
        "\n",
        "The unique disentanglement of features in StyleGAN allows us to mix different models and create a Star Wars like character. If the first few layers control the facial features and the last layers contain the texture, what happens if we swap the last layers with the layers of another model?\n",
        "For example, if we use the face model's weights for the first few layers and the model Star Wars 's weights for the remaining layers, the face will be created with the Star Wars style! Also, the texture of the second model and the type of facial features of the different models can be copied, e.g. the eye or the mouth of the Star Wars character\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBqnK_63l7-j"
      },
      "source": [
        "Okay, let's get started with the project, first clone the repository and install all the necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpquwmoSmaFg",
        "outputId": "6ace96d6-fd43-41c4-ce58-6ca8d011b971"
      },
      "source": [
        "!git clone https://github.com/ferdinandl007/Northern-Data-Star-Wars-GAN.git\n",
        "%cd /content/Northern-Data-Star-Wars-GAN/DatasetGenerator\n",
        "!pip install --upgrade pip\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Northern-Data-Star-Wars-GAN'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 145 (delta 16), reused 27 (delta 10), pack-reused 104\u001b[K\n",
            "Receiving objects: 100% (145/145), 104.00 MiB | 53.65 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "/content/Northern-Data-Star-Wars-GAN/DatasetGenerator\n",
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/ef/60d7ba03b5c442309ef42e7d69959f73aacccd0d86008362a681c4698e83/pip-21.0.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5MB 20.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-21.0.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.19.5)\n",
            "Collecting pytubeX\n",
            "  Downloading pytubeX-0.1.4.tar.gz (47 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting shortuuid\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.1.2.30)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.6.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 5)) (53.0.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 5)) (8.7.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 5)) (20.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 5)) (1.10.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from pytubeX->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Building wheels for collected packages: pytubeX\n",
            "  Building wheel for pytubeX (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytubeX: filename=pytubeX-0.1.4-py3-none-any.whl size=38534 sha256=09700c4d02cf99b06e28a1d8212753cc74e12ba240439883fb04e18fed73455e\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/82/da/83e510fed90fbe5cb4c759b40e1e625ba284b4f6148451b656\n",
            "Successfully built pytubeX\n",
            "Installing collected packages: shortuuid, pytubeX\n",
            "Successfully installed pytubeX-0.1.4 shortuuid-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOGvKlFG6OmU"
      },
      "source": [
        "# Data Generation\n",
        "The project includes a DataSet generation tool that can generate a DataSet of faces from YouTube videos - all you need to do is enter the URLs, and it will do the rest\n",
        "At this step, you have three options: You can either generate a new dataset with your own custom URLs, but make sure you provide at least three hours of video content, or you can generate the dataset with the default URLs of Star Wars the clone wars videos, and finally, you can just download all the process data I prepared for you!\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u--c2pUN24gN"
      },
      "source": [
        "from GenerateDataset import GenerateDataset\n",
        "you_video_urls = []\n",
        "dataset_generator = GenerateDataset(every_x_frame=13, set_fix_image_size=256, video_urls=you_video_urls)\n",
        "dataset_generator.run()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1shNH03ToyOp"
      },
      "source": [
        "from GenerateDataset import GenerateDataset\n",
        "dataset_generator = GenerateDataset(every_x_frame=13, set_fix_image_size=256)\n",
        "dataset_generator.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTQ_EgYA6huq"
      },
      "source": [
        "Alternatively download the Star Wars the clone wars DataSet here!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW9rZVs86VgH"
      },
      "source": [
        "%cd /content/Northern-Data-Star-Wars-GAN/DatasetGenerator/\n",
        "!wget https://www.dropbox.com/s/v6ffbfof26byuy1/faces.zip\n",
        "from zipfile import ZipFile\n",
        "zf = ZipFile('faces.zip', 'r')\n",
        "zf.extractall()\n",
        "zf.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoAHbE_aqGD4"
      },
      "source": [
        "# DataSet preparation\n",
        "Now that we have the dataset ready, we need to convert it to a DataFrame that Tenser Flow and styleGan can understand - in this case, it's a tfrecord\n",
        "![Alt Text](https://storage.googleapis.com/kf-pipeline-contrib-public/release-0.1.1/kfp-components/data_converter/preprocessing_image/assets/image.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD9MvVtL6nx3"
      },
      "source": [
        "%cd /content/Northern-Data-Star-Wars-GAN/DatasetGenerator/faces\n",
        "!python /content/Northern-Data-Star-Wars-GAN/stylegan2/dataset_tool.py create_from_images_raw /content/Northern-Data-Star-Wars-GAN/stylegan2/dataset/starWars ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrOOhJ_Mqsey"
      },
      "source": [
        "# training custom styleGan network\n",
        "Finally we get to the interesting part, training your own styleGan network. But beware, this may take a while ðŸ˜¬. For are project, we want to tune a network that was previously trained to generate human faces so that it now generates Star Wars characters that we can later use for network blending.\n",
        "\n",
        "\n",
        "As mentioned earlier, Justin Pinkney introduced a concept I called \"layer swapping\" to mix two StyleGAN models. The goal was to mix a base model and another that was created from it using transfer learning, the fine-tuned model. The method was different than simply interpolating the weights of the two models because it allows you to independently control which model you get the low and high resolution features from; \n",
        "Here is an example using this technique to get the pose from normal photos and the texture/style from ukiyo-e prints\n",
        "![link text](https://www.justinpinkney.com/static/eab0821e721edd413d95de86fc04b35a/b17f8/mr79.jpg)\n",
        "\n",
        "\n",
        "Note: Once the training is running, you should cheque the results in \"/content/Northern-Data-Star-Wars- GAN /results\" and stop the training if you are satisfied with the results. \n",
        "Then copy the last \".pkl\" into the directory stylegan2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4vcJRyyDnF8"
      },
      "source": [
        "# download pre-trained model for transfer learning\n",
        "%cd /content/Northern-Data-Star-Wars-GAN/stylegan2\n",
        "!wget https://www.dropbox.com/s/f7ts2pdg7ml2fjd/ffhq-256-config-e-003810.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYWCUqGhTDMG"
      },
      "source": [
        "%cd /content/Northern-Data-Star-Wars-GAN/stylegan2\n",
        "!python3 run_training.py --num-gpus=1 --data-dir=./dataset --config=config-f --minibatch-gpu-base=8 --dataset=starWars --min-h=3 --min-w=3 --res-log2=7 --mirror-augment=true --metric=none --total-kimg=500 --resume-pkl=\"/content/Northern-Data-Star-Wars-GAN/stylegan2/ffhq-256-config-e-003810.pkl\" --result-dir=\"/content/Northern-Data-Star-Wars-GAN/results\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "It0FeTDBxQeq",
        "outputId": "4bfd9aa1-879f-4c24-9bf3-a4903eac95c9"
      },
      "source": [
        "%%html\n",
        "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Blending humans and cartoons using  @Buntworthy's Google Colab notebook. Thank you for that, it's awesome. Here is a YouTube version of this video: https://youtu.be/W-VRdE7zagw. <a href=\"https://t.co/qO5ySH2TxT\">pic.twitter.com/qO5ySH2TxT</a></p>&mdash; Justin (@Buntworthy) <a href=\"https://twitter.com/Norod78/status/1297513475258953728?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1297513475258953728%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwww.justinpinkney.com%2Fstylegan-network-blending%2F\">August 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Blending humans and cartoons using  @Buntworthy's Google Colab notebook. Thank you for that, it's awesome. Here is a YouTube version of this video: https://youtu.be/W-VRdE7zagw. <a href=\"https://t.co/qO5ySH2TxT\">pic.twitter.com/qO5ySH2TxT</a></p>&mdash; Justin (@Buntworthy) <a href=\"https://twitter.com/Norod78/status/1297513475258953728?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1297513475258953728%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwww.justinpinkney.com%2Fstylegan-network-blending%2F\">August 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ympW-I82yD3P"
      },
      "source": [
        "# Network blending\n",
        "It's time to try out some network blending for ourselves. Let try some different resolutions for the switch and see the results. Remember that the Star Wars model provides the low resolution layers, i.e. the \"structure\", and the original surface model provides the high resolution, i.e. the \"texture\"\n",
        "\n",
        "I'll run the main function in blend_models.py in a Python loop, but you can also run it from the command line, like this\n",
        "\n",
        "Note: make sure to change \"starWars.pkl\" the path of your own trend \".pkl\" file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9u-78DxEIjF"
      },
      "source": [
        " # downloading the base face model we want to blend with\n",
        " !wget https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-ffhq.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpruRzq6SqUp"
      },
      "source": [
        "%cd /content/Northern-Data-Star-Wars-GAN/stylegan2\n",
        "from IPython.display import Image \n",
        "import blend_models\n",
        "resolutions = (8, 16, 32, 64, 128)\n",
        "for res in resolutions:\n",
        "  filename = f\"blended-{res}.jpg\"\n",
        "  blend_models.main(\"starWars.pkl\", \"stylegan2-ffhq.pkl\", res, output_grid=filename)\n",
        "  img = Image(filename=filename)\n",
        "  print(f\"blending at {res}x{res}\")\n",
        "  display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcwLX5UozELU"
      },
      "source": [
        "# Save blended model\n",
        "When you are satisfied with the blended models you have just created, choose the resolution you like best and export it as the final blended model\n",
        "\n",
        "Note: make sure to change \"starWars.pkl\" the path of your own trend \".pkl\" file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBKfzVnLSqUp",
        "outputId": "74d193d3-69f2-4364-e8a8-b74ae8a8318f"
      },
      "source": [
        "!python3 blend_models.py starWars.pkl stylegan2-ffhq.pkl 32 --output-pkl=\"blended.pkl\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'blend_models': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O507XB1w2Iu_"
      },
      "source": [
        "Let's make some directories which we will need later on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhfZDGvWSqUq"
      },
      "source": [
        "!mkdir aligned\n",
        "!mkdir generate\n",
        "!mkdir raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHlJdSBG2oXA"
      },
      "source": [
        "# download image\n",
        "Let's download a picture we Star Warsify, if you want to Star Warsify yourself, replace the URL with one that points to a picture of yourself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRLIMjAzSqUq"
      },
      "source": [
        "!wget https://www.bondguide.de/wp-content/uploads/2021/02/Aroosh-Thillainathan-Northern-Data.jpg -O raw/example.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHZ3Kcx_3x2d"
      },
      "source": [
        "All we need to do now is some alignment to crop your face out of this picture so we get a good result ðŸ˜‰."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVPPuhikSqUq"
      },
      "source": [
        "!python align_images.py raw aligned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F46SKFsL4OQG"
      },
      "source": [
        "# create a projection of your face in latent space\n",
        "So far we've generated random faces from random initialised latent vectors in space, but now we want to turn your face into a Star Wars character, so what do we do?\n",
        "We use stylegan2 to project your face into the latent space\n",
        "\n",
        "## What is the styleGAN2 projection?\n",
        "The method is intelligent. StyleGAN projects a given image into latent space - the origins of the generated images.\n",
        "It has some superficial similarities to reverse image search (like using TinEye or Google Image Search to find out if the image is a stock photo\n",
        "\n",
        "StyleGAN2 Projection compares the image in question to the latent space where GAN images are generated, and searches for the origins. In theory. In practise, it's... complicated.\n",
        "So we won't go into it, but essentially the face is projected into the latent space, which gives us the vector to recreate your face with StyleGAN, then we take the vector and feed it through a blended model, which gives us your face with Star Wars features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hyw1rKnSqUq"
      },
      "source": [
        "!python project_images.py --num-steps 500 aligned generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X08C7FNFM-Y"
      },
      "source": [
        "# Finally let's generate your Star Warsify Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A3PV-xvSqUr"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "from pathlib import Path\n",
        "import pretrained_networks\n",
        "\n",
        "# use my copy of the blended model to save Doron's download bandwidth\n",
        "# get the original here https://mega.nz/folder/OtllzJwa#C947mCCdEfMCRTWnDcs4qw\n",
        "blended_url = \"blended.pkl\" \n",
        "ffhq_url = \"stylegan2-ffhq.pkl\"\n",
        "\n",
        "_, _, Gs_blended = pretrained_networks.load_networks(blended_url)\n",
        "_, _, Gs = pretrained_networks.load_networks(ffhq_url)\n",
        "\n",
        "latent_dir = Path(\"generated\")\n",
        "latents = latent_dir.glob(\"*.npy\")\n",
        "for latent_file in latents:\n",
        "  latent = np.load(latent_file)\n",
        "  latent = np.expand_dims(latent,axis=0)\n",
        "  synthesis_kwargs = dict(output_transform=dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=False), minibatch_size=8)\n",
        "  images = Gs_blended.components.synthesis.run(latent, randomize_noise=False, **synthesis_kwargs)\n",
        "  Image.fromarray(images.transpose((0,2,3,1))[0], 'RGB').save(latent_file.parent / (f\"{latent_file.stem}-toon.jpg\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnbh78qFSqUr"
      },
      "source": [
        "from IPython.display import Image \n",
        "embedded = Image(filename=\"generated/example_01.png\", width=256)\n",
        "display(embedded)\n",
        "tooned = Image(filename=\"generated/example_01-toon.jpg\", width=256)\n",
        "display(tooned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRHUI9M4Ffu6"
      },
      "source": [
        "# Going further with animations!\n",
        "\n",
        "To round off this ML example, let's animate this image  with Pytorch and Aliaksanr first order motion "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1IGyrWGGIic"
      },
      "source": [
        "%cd /content/Northern-Data-Star-Wars-GAN/stylegan2\n",
        "!git clone https://github.com/AliaksandrSiarohin/first-order-model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p9xzQdJGYPo"
      },
      "source": [
        "try: # set up path\n",
        "    import sys\n",
        "    sys.path.append('/content/Northern-Data-Star-Wars-GAN/stylegan2/first-order-model')\n",
        "    print('Path added')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AH14lpPGgAY"
      },
      "source": [
        "Now we download the pre-trained weights for the keypoint detector and the video generator. The file is about 600 MB and may take a while to download. If the wget download fails, you can manually download the file and move it to your colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "parZRMjFGaxy"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?export=download&id=1jmcn19-c3p8mf39aYNXUhdMqzqDYZhQ_\" -O vox-cpk.pth.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGsnTDEkGklQ"
      },
      "source": [
        "Load the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmFIZmwdGmw_"
      },
      "source": [
        "from demo import load_checkpoints\n",
        "generator, kp_detector = load_checkpoints(config_path='first-order-model/config/vox-256.yaml', \n",
        "                            checkpoint_path='vox-cpk.pth.tar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxg0qy9ZGvjw"
      },
      "source": [
        "Download the source video, you can use your own or use the sample video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqqQS8NgGuN0"
      },
      "source": [
        "%cd /content/Northern-Data-Star-Wars-GAN/stylegan2\n",
        "!wget https://www.dropbox.com/s/s9nrzq9hqevejrh/Bellapoarch%20TikTok%20_It_s%20M%20To%20The%20B%22.mp4 -O src_video.mp4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOrUv7aWNrD2"
      },
      "source": [
        "# To conclude this notebook, we generate and display the animation. Feel free to download it and share it on social media!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOEK7t9zNXBs"
      },
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage.transform import resize\n",
        "from IPython.display import HTML\n",
        "\n",
        "from demo import make_animation\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "source_image = imageio.imread('generated/example_01-toon.jpg')\n",
        "reader = imageio.get_reader('src_video.mp4')\n",
        "\n",
        "\n",
        "#Resize image and video to 256x256\n",
        "\n",
        "source_image = resize(source_image, (256, 256))[..., :3]\n",
        "\n",
        "fps = reader.get_meta_data()['fps']\n",
        "driving_video = []\n",
        "try:\n",
        "    for im in reader:\n",
        "        driving_video.append(im)\n",
        "except RuntimeError:\n",
        "    pass\n",
        "reader.close()\n",
        "\n",
        "driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
        "\n",
        "# Generate animation\n",
        "predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=True)\n",
        "\n",
        "def display(source, driving, generated=None):\n",
        "    fig = plt.figure(figsize=(8 + 4 * (generated is not None), 6))\n",
        "\n",
        "    ims = []\n",
        "    for i in range(len(driving)):\n",
        "        cols = [source]\n",
        "        cols.append(driving[i])\n",
        "        if generated is not None:\n",
        "            cols.append(generated[i])\n",
        "        im = plt.imshow(np.concatenate(cols, axis=1), animated=True)\n",
        "        plt.axis('off')\n",
        "        ims.append([im])\n",
        "\n",
        "    ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n",
        "    plt.close()\n",
        "    return ani\n",
        "    \n",
        "\n",
        "HTML(display(source_image, driving_video, predictions).to_html5_video())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}